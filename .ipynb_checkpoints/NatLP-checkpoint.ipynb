{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAP from restaurant name to list [filtered reviews in a dataframe, list of menu items]\n",
    "def big_object():\n",
    "    mapping  = {}\n",
    "    for restaurant in os.listdir(\"menu_items/\")[1:]: #get rid of the first DS_Store name TAKE OUT THE 2\n",
    "        name = restaurant[:-4]\n",
    "        menu = []\n",
    "        try:\n",
    "            reviews = pd.read_json(\"reviews/\"+name+\".json\",lines = True)\n",
    "        except:\n",
    "            continue\n",
    "        with open(\"menu_items/\" + restaurant) as f:\n",
    "            content = f.read().splitlines()\n",
    "            for item in content:\n",
    "                menu.append(''.join(re.findall(r'[\\w\\d]*[\\s]*[\\w\\d]*[^\\s]', item)))\n",
    "        mapping[name] = [reviews, menu]\n",
    "    for restaurant in mapping:\n",
    "        mapping[restaurant][0]['rating'] = mapping[restaurant][0][\"reviewRating\"].apply(func = lambda x: x[\"ratingValue\"])\n",
    "        mapping[restaurant][0] = mapping[restaurant][0][[\"description\", \"rating\"]]\n",
    "        mapping[restaurant][0] = mapping[restaurant][0].rename(index = str, columns = {'description':'review'})\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp data for everyone else except John\n",
    "test_data = mapping[\"bi-rite-creamery-san-francisco\"]\n",
    "data = test_data[0]\n",
    "menu_items = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT BIG DATA HERE WHEN AFTER DONE TESTING\n",
    "#stop words\n",
    "sw = set([w for w in stopwords.words(\"english\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-f8b20bf043cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#temp data when John runs because his disk drive is full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bi-rite-creamery-san-francisco.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m menu_items = ['A Scoop Of Each',\n\u001b[1;32m      4\u001b[0m  \u001b[0;34m'Afternoon Snack'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0;34m'Balsamic Strawberry'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data-x/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data-x/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             obj = self._get_object_parser(\n\u001b[0;32m--> 526\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m             )\n\u001b[1;32m    528\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data-x/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'frame'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data-x/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data-x/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 853\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    854\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             decoded = {str(k): v for k, v in compat.iteritems(\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "#temp data when John runs because his disk drive is full\n",
    "data = pd.read_json(\"bi-rite-creamery-san-francisco.json\", lines=True)\n",
    "menu_items = ['A Scoop Of Each',\n",
    " 'Afternoon Snack',\n",
    " 'Balsamic Strawberry',\n",
    " 'Banana Split',\n",
    " 'Bars',\n",
    " 'Basil',\n",
    " 'Berry Sundae',\n",
    " 'Brown Sugar',\n",
    " 'Brownie Sundae',\n",
    " 'Carrot Cake',\n",
    " 'Chocolate',\n",
    " 'Chocolate Cake',\n",
    " 'Chocolate Chip Cookies',\n",
    " 'Chocolate Coconut (vegan)',\n",
    " 'Coffee Toffee',\n",
    " 'Cookies',\n",
    " 'Cookies And Cream',\n",
    " 'Cupcakes',\n",
    " 'Dainty Gentleman',\n",
    " 'Honey Lavender',\n",
    " 'Malted Vanilla',\n",
    " 'Malted Vanilla With Peanut Brittle & Chocolate',\n",
    " 'Mint Chip',\n",
    " 'Olive Oil',\n",
    " 'Ricanelas',\n",
    " 'Ricanelas Ice Cream',\n",
    " 'Roasted Banana',\n",
    " \"S'more Ice Cream Pie\",\n",
    " 'Salted Caramel',\n",
    " \"Sam's Sundae\",\n",
    " 'Sugar Cookie',\n",
    " 'Toasted Coconut',\n",
    " 'Toasted Coconut Ice Cream',\n",
    " 'Trifecta',\n",
    " 'Vanilla',\n",
    " 'White Chocolate Raspberry Swirl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleans the dish name of punctuation\n",
    "def clean_menu_item(dish):\n",
    "    # TODO: append \n",
    "    dish = dish.lower()\n",
    "    dish = re.sub(\"[\\'\\\".!?\\\\-\\n\\/]\", \"\", dish)\n",
    "    return dish\n",
    "\n",
    "#Checks if the sentence contains a wording similar to a menu item\n",
    "def contains(sentence, menu_items):\n",
    "    most_likely_match = 0\n",
    "    most_likely_length = 0\n",
    "    complete_matches = []\n",
    "    for item in menu_items:\n",
    "        item_words = [w for w in item.split(\" \") if w not in sw]\n",
    "        match = sum([1 for word in item_words if word.lower() in sentence])/len(item_words)\n",
    "        if match > most_likely_match:\n",
    "            most_likely_match = match\n",
    "            most_likely_length = len(item_words)\n",
    "            most_likely_dish = item\n",
    "        elif match == most_likely_match and len(item_words) >= most_likely_length:\n",
    "            most_likely_match = match\n",
    "            most_likely_length = len(item_words)\n",
    "            most_likely_dish = item\n",
    "    if most_likely_length < 4 and most_likely_match != 1: \n",
    "        return \"\"\n",
    "    elif most_likely_match <= 0.75:\n",
    "        return \"\"\n",
    "    return most_likely_dish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a dictionary mapping menu item to a list of sentences mentioning it. \n",
    "def generate_dish_reviews(data, menu):\n",
    "    reviews = data['review']\n",
    "    \n",
    "    menu_items = [clean_menu_item(dish) for dish in menu]\n",
    "    dish_reviews = {dish:[0,0] for dish in menu_items}\n",
    "    index = 0\n",
    "    for r in list(reviews):\n",
    "        sentences = sent_tokenize(r)\n",
    "        #assuming that one food is mentioned per sentence\n",
    "        curr_food = None;\n",
    "        mentioned_foods = set()\n",
    "        counter = 0;\n",
    "        for sentence in sentences:\n",
    "            sentence = re.sub(\"[\\n]\", \" \", sentence)\n",
    "            sentence = sentence.lower()\n",
    "            dish = contains(sentence, menu_items)\n",
    "            if dish:\n",
    "                if curr_food != dish:\n",
    "                    curr_food = dish\n",
    "            if curr_food == None:\n",
    "                continue\n",
    "            if counter < 2:\n",
    "                dish_reviews[curr_food].append(sentence)\n",
    "                mentioned_foods.add(curr_food)\n",
    "                counter += 1\n",
    "            else:\n",
    "                counter = 0 \n",
    "        for food in mentioned_foods:\n",
    "            dish_reviews[food][0] += data.rating[index]\n",
    "            dish_reviews[food][1] += 1\n",
    "        #get average rating\n",
    "        index += 1\n",
    "        \n",
    "        \n",
    "    return dish_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rating(mapping, data):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    dish_ratings = {}\n",
    "    for dish in mapping:\n",
    "        reviews = mapping[dish]\n",
    "        \n",
    "        comp_scores = [analyser.polarity_scores(review)['compound'] for review in reviews[2:]]\n",
    "        if len(comp_scores) == 0:\n",
    "            continue\n",
    "\n",
    "        avg_comp = sum(comp_scores)/len(comp_scores)\n",
    "    \n",
    "        pos_scores = [analyser.polarity_scores(review)['pos'] for review in reviews[2:]]\n",
    "    \n",
    "        avg_pos = sum(pos_scores)/len(pos_scores)\n",
    "    \n",
    "        neg_scores = [analyser.polarity_scores(review)['neg'] for review in reviews[2:]]\n",
    "        avg_neg = sum(neg_scores)/len(neg_scores)\n",
    "    \n",
    "        sentiment_score = 2 * avg_comp + avg_pos - avg_neg\n",
    "    \n",
    "        appearance_score = np.log(len(reviews))\n",
    "        \n",
    "        average_rating = reviews[0]/reviews[1]\n",
    "        dish_ratings[dish] = 0.45 * sentiment_score + 0.30 * appearance_score + 0.25 * average_rating\n",
    "    dish_ratings = sorted(dish_ratings.items(), key = lambda kv: kv[1], reverse = True)\n",
    "        \n",
    "    return dish_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    top_five = {}\n",
    "    mapping = big_object()\n",
    "    count  = 3\n",
    "    for restaurant in mapping:\n",
    "        test_data = mapping[restaurant]\n",
    "        data = test_data[0]\n",
    "        menu_items = test_data[1]\n",
    "        scores = calculate_rating(generate_dish_reviews(data, menu_items), data)\n",
    "        top_five[restaurant] = [dish[0] for dish in scores]\n",
    "        count-=1\n",
    "        if count == 0: break\n",
    "    return top_five\n",
    "jsonobj = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('FINAL_OUTPUT.json', 'w') as outfile:\n",
    "    json.dump(jsonobj, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
